# See: "Monoaural Audio Source Separation Using Deep Convolutional Neural Networks" - Pritish Chandna
# https://github.com/MTG/DeepConvSep/blob/master/examples/dsd100/separate_dsd.py
#
# and: "Fast Music Source Separation" - Leonardo Pepino
#

# changes:
#
# for memory:
# - filters 50 => 20
# - conv/deconv: 1025 => 129
#
# for time:
# - filters 50 => 8
#
# - changed the split positions
# - added 1 batchenorm at the end
# - glorot_uniform
#
# NOTE: kernel_initializer=random_uniform is the best (tested default, and glorot_uniform)
#
# - used batchnorm=1 everywhere => now the vocal doesn't disappear anymore
#

[net]
# Training
width=1024
height=32

# multichannel
channels=4

# origin
#learning_rate=0.01

# test (1 single data sample)
# => totally mess
#learning_rate=0.1

# ORIGIN
#learning_rate=0.00001
#learning_rate=0.01
#learning_rate=0.00001 # origin
#learning_rate=0.01
#learning_rate=0.00001 # origin

# test (1 single data sample)
# => seems to be a bit worse than 1e-5
#learning_rate=0.0001

# test (1 single data sample)
# => increased less quickly,
# and finally converge to exactly the same limit as 1e-5
#learning_rate=0.0000001

#?
batch=1
# test
#batch=2
#batch=5

momentum=0.9
# ?
decay=0.0001 

adam=1
B1=0.9
B2=0.999
eps=1e-7

learning_rate=0.001

# TODO: implement it in darknet
clip=0.9

max_batches = 10000000

policy=constant

#0
#[input]

# must take only 1 channel of the input gray image (r=g=b=a)
# otherwise the first convolution won't work well
# (would have 4 times the same filter)
#0
[split]
channel=0

#1
[batchnorm]
global=1
#global=0

#2
[convolutional2] 
filters=4
size_x=33
size_y=1
stride_x=1
stride_y=1
pad=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#3
[batchnorm]
global=1
#global=0

#4
[convolutional2] 
filters=4
size_x=1
size_y=5
stride_x=1
stride_y=1
pad=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#5
[batchnorm]
global=1
#global=0

# contraction
# DeepConvSep set it to small value
# Pepino sets to 1024
#
# In any case, we must set 128 or 512 values, here or in each branch.
# Expl: Since we have 1 channel input (instead of 4), there must be a factor of
# 4 between the pairs of dense layers (either 512->128, or 128->512).
# Otherwise we get empty weights for deconvolution
#6 
[dense]
#units=128
units=32
kernel_initializer=random_uniform
#batch_normalize=1

#7
[batchnorm]
global=1
#global=0

########## source0
# DeepConvSep says to set the second dense dimensions to the ones of the previous conv (the second one)
# Pepino sets to smaller value, for real-time
#8
[dense]
#units=512
units=128
kernel_initializer=random_uniform
#batch_normalize=1

#9
[batchnorm]
global=1
#global=0

#10
[deconvolutional2] 
filters=4
size_x=1
size_y=5
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#11
[batchnorm]
global=1
#global=0

#12
[deconvolutional2] 
filters=4
size_x=33
size_y=1
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#13
[batchnorm]
global=1
#global=0

#### end source0

########## source1
#14
[route]
#layers=6
layers=7

#15
[dense]
#units=512
units=128
kernel_initializer=random_uniform
#batch_normalize=1

#16
[batchnorm]
global=1
#global=0

#17
[deconvolutional2] 
filters=4
size_x=1
size_y=5
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#18
[batchnorm]
global=1
#global=0

#19
[deconvolutional2] 
filters=4
size_x=33
size_y=1
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#20
[batchnorm]
global=1
#global=0

#### end source1

########## source2
#21
[route]
#layers=6
layers=7

#22
[dense]
#units=512
units=128
kernel_initializer=random_uniform
#batch_normalize=1

#23
[batchnorm]
global=1
#global=0

#24
[deconvolutional2] 
filters=4
size_x=1
size_y=5
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#25
[batchnorm]
global=1
#global=0

#26
[deconvolutional2] 
filters=4
size_x=33
size_y=1
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#27
[batchnorm]
global=1
#global=0

#### end source2

########## source3
#28
[route]
#layers=6
layers=7

#29
[dense]
#units=512
units=128
kernel_initializer=random_uniform
#batch_normalize=1

#30
[batchnorm]
global=1
#global=0

#31
[deconvolutional2] 
filters=4
size_x=1
size_y=5
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#32
[batchnorm]
global=1
#global=0

#33
[deconvolutional2] 
filters=4
size_x=33
size_y=1
stride_x=1
stride_y=1
pad=1
padding_x=1
padding_y=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1

#34
[batchnorm]
global=1
#global=0

#### end source3

#35
[route]
#layers=9,13,17,21
layers=13,20,27,34

#36
[batchnorm]
global=1
#global=0

# NOTE: transform 4x4 channels data to 4 channel data
#37
[convolutional2] 
filters=4
size_x=3
size_y=3
stride_x=1
stride_y=1
pad=1
activation=relu
kernel_initializer=random_uniform
#batch_normalize=1
# TEST
groups=4

#38 #very important!
[batchnorm]
global=1
#global=0

#39
[metrics]
# used successfully for prev unet
# but here, it doesn't work..
#type_cost=L1mask

# used by Pepino
# => works the best!
#type_cost=MSEmask
#type_acc=MSEmask
#
# Good
type_cost=L1mask
type_acc=L1mask
